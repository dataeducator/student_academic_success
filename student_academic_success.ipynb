{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a5ae190",
   "metadata": {},
   "source": [
    "# Final Project Submission: Student Academic Success Classification Model Analysis\n",
    "__(Phase 3)__\n",
    "\n",
    "* Student Name: Tenicka Norwood\n",
    "* Program Pace: self paced\n",
    "* Scheduled Project Review Time: \n",
    "* Instructor name: Morgan Jones\n",
    "* Blog post Url: https://medium.com/@tenicka.norwood/the-meaning-of-life-the-universe-and-everything-9423dcb79c06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58442a5b",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f98bb5c",
   "metadata": {},
   "source": [
    "* __Stakeholder__: Instituto Politecnico de Portalegre\n",
    "* __Business Case__: I have been tasked by the Instituto Politecnico de Portalegre to accurately classify students' academic success. Researchers at the Instituto Politecnico de Portalegre want to reduce the rate of student academic failure in higher education. \n",
    "\n",
    "\n",
    "In this case, the goal is craft a reliable model that can be refined over time as more information becomes available by using machine learning techniques to identify which students are at risk at ealier stages of their academic path in order to put strategies to support students in place to mitigate their likelihood of dropping out of higher education.While the model will be created with data from higher education instutionstudents in Portugal, the lag in academic success within higher education is also a challenge within the United States. Colleges and Universities face the ever pressing challgence of identifying students who are at risk of not graduating on time and providing effective interventions to move those students back onto a positive pathway to graduation. \n",
    "> The completion rates of (US) undergraduates within six years of enrollment stand at only __62.3%__ as of 2022.\n",
    ">\n",
    "> -- Hanneh Bareham and Chelsea Wing (bankrate.com)\n",
    ">\n",
    "> \n",
    ">\n",
    "\n",
    "Carnevale et. al. (2021) Georgetown University in their study highlighted the substantial disparity in lifetime earnings based on educational attainment. In particular, the gist of this study showed that an average person with only a high school diploma or GED will earn an estimated  __1.6  million dollars__ in their lifetime, compared to a person with a bachelor's degree who potentially can earn ~__2.8 million dollars__ in their lifetime. This profound disparity in earning potential underscores the transformative power of higher education and highlights the significance of timely graduation in enhancing students' long-term financial security.\n",
    "\n",
    "\n",
    "###  Objectives\n",
    "We will used the __OSEMiN__ pipeline to:\n",
    "* Obtain &rarr; Import the data.\n",
    "* Scrub &rarr; Manage the datatypes, resolve missing data or duplicates.\n",
    "* Explore &rarr; Identify patterns within the relationships between variables in the data.\n",
    "* Model &rarr; Create a set of predictive models.\n",
    "* iNterpret &rarr; Identify insights and create visualiations of findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f7ba2",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b6e2d",
   "metadata": {},
   "source": [
    "The University of California (Irvine) hosts a machine learning repository with datasets that can be evaluated using machine learning techniques. The dataset used in this exploration can be found [here](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success). This datset was created as a part of a project to reduce academic dropout and failure rates in higher edcuation by leveraging machine learning to identify at risk students early within their academic careers and provide them support to improve their likelihood of on time graduation.  \n",
    "\n",
    "The data includes information that is known from the time a student enrolls including their:\n",
    "- academic path\n",
    "- demographics\n",
    "- social-economic factors\n",
    "\n",
    "There are three classifications of students within this dataset:\n",
    "- dropout\n",
    "- enrolled\n",
    "- graduate\n",
    "\n",
    "The data was preprocessed to address missing values and anomalies. The dataset has __37__ columns with __4424__ rows of data. I am going to use a minimum of 4 machine learning algorithms to classify this data and will also use exploration to determine: \n",
    "\n",
    "- How does marital status influece the likelihoon of students graduating on time?\n",
    "- Does the timely payment of tuition fees have any impact on the graduation status of students?\n",
    "- Is there are relationship between the parents' qualifications and the graduation status of students?\n",
    "\n",
    "### Features\n",
    "\n",
    "|Name | Role | Type| Data Type   |Description | \n",
    "|-------|---------|----- |------------- |----------|\n",
    "|Marital status| Feature| Discrete|int|1 &mdash; single 2 &mdash; married 3 &mdash; widowed 4 &mdash; divorced 5 &mdash; facto union  6 &mdash; legally separated| \n",
    "|Application mode|Feature|Discrete|int |1 &ndash; 1st phase &ndash; general contingent 2 &ndash;Ordinance No. 612/93 <br> 5 &ndash; 1st phase &ndash; special contingent (Azores Island) 7&ndash; Holders of other higher courses 10 &ndash; Ordinance No. 854-B/99 15 &ndash;International student (bachelor) 16 &ndash; 1st phase &ndash; special contingent (Madeira Island) 17 &ndash; 2nd phase &ndash; general contingent 18 &ndash; 3rd phase &ndash; general contingent 26 &ndash; Ordinance No. 533-A/99, item b2) (Different Plan) 27 - Ordinance No. 533-A/99, item b3 (Other Institution) 39 &ndash; Over 23 years old 42 &ndash; Transfer 43 &ndash; Change of course 44 &ndash; Technological specialization diploma holders 51 &ndash; Change of institution/course 53 &ndash; Short cycle diploma holders 57 &ndash; Change of institution/course (International)|\n",
    "|Application order|Feature|Discrete|int|Application order <br> between 0 - first choice; and 9 last choice|\n",
    "|Course|Feature|Discrete|int |33 - Biofuel Production Technologies 171 - Animation and Multimedia Design 8014 - Social Service (evening attendance) 9003 - Agronomy 9070 - Communication Design 9085 - Veterinary Nursing 9119 - Informatics Engineering 9130 - Equinculture 9147 - Management 9238 - Social Service 9254 - Tourism 9500 - Nursing 9556 - Oral Hygiene 9670 - Advertising and Marketing Management 9773 - Journalism and Communication 9853 - Basic Education 9991 - Management (evening attendance)|\t\n",
    "|Daytime/evening attendance|Feature|Discrete|int|1 &mdash; daytime 0 &mdash; evening|\n",
    "|Previous qualification|Feature|Discrete|int|1  &ndash; Secondary education 2  &ndash; Higher education - bachelor's degree 3  &ndash; Higher education - degree 4  &ndash; Higher education - master's    <br> 5&ndash; Higher education - doctorate   6  &ndash; Frequency of higher education 9 - 12th year of schooling - not completed 10 - 11th year of schooling - not completed 12  &ndash; Other  &ndash; 11th year of schooling  14  &ndash; 10th year of schooling 15  &ndash; 10th year of schooling - not completed 19 - Basic education 3rd cycle (9th/10th/11th year) or equiv. 38  &ndash; Basic education 2nd cycle (6th/7th/8th year) or equiv. <br>39  &ndash; Technological specialization course 40  &ndash; Higher education - degree (1st cycle) 42  &ndash; Professional higher technical course<br> 43  &ndash; Higher education - master (2nd cycle)|\n",
    "|Previous qualification (grade)|Feature|Continuous|float|Grade of previous qualification <br> between 0 and 200|\n",
    "|Nacionality|Feature|Discrete||1 &mdash; Portuguese; 2 &mdash; German; 6 &mdash; Spanish; 11 &mdash; Italian; 13 &mdash; Dutch; 14 &mdash; English; 17 &mdash; Lithuanian; 21 &mdash; Angolan; <br>22 &mdash; Cape Verdean; 24 &mdash; Guinean; 25 &mdash; Mozambican; 26 &mdash; Santomean; 32 &mdash; Turkish; 41 &mdash; Brazilian; 62 &mdash; Romanian; <br> 100 &mdash; Moldova (Republic of); 101 &mdash; Mexican; 103 &mdash; Ukrainian; 105 &mdash; Russian; 108 &mdash; Cuban; 109 &mdash; Colombian|\n",
    "|Mother's qualification|Feature|Discrete|int|1 - Secondary Education - 12th Year of Schooling or Eq. 2 - Higher Education - Bachelor's Degree 3 - Higher Education - Degree 4 - Higher Education - Master's 5 - Higher Education - Doctorate 6 - Frequency of Higher Education 9 - 12th Year of Schooling - Not Completed 10 - 11th Year of Schooling - Not Completed 11 - 7th Year (Old) 12 - Other - 11th Year of Schooling 14 - 10th Year of Schooling 18 - General commerce course 19 - Basic Education 3rd Cycle (9th/10th/11th Year) or Equiv. 22 - Technical-professional course 26 - 7th year of schooling 27 - 2nd cycle of the general high school course 29 - 9th Year of Schooling - Not Completed 30 - 8th year of schooling 34 - Unknown 35 - Can't read or write 36 - Can read without having a 4th year of schooling 37 - Basic education 1st cycle (4th/5th year) or equiv. 38 - Basic Education 2nd Cycle (6th/7th/8th Year) or Equiv. 39 - Technological specialization course 40 - Higher education - degree (1st cycle) 41 - Specialized higher studies course 42 - Professional higher technical course 43 - Higher Education - Master (2nd cycle) 44 - Higher Education - Doctorate (3rd cycle)|\n",
    "|Father's qualification|Feature|Discrete|int|\t1 - Secondary Education - 12th Year of Schooling or Eq. 2 - Higher Education - Bachelor's Degree 3 - Higher Education - Degree 4 - Higher Education - Master's 5 - Higher Education - Doctorate 6 - Frequency of Higher Education 9 - 12th Year of Schooling - Not Completed 10 - 11th Year of Schooling - Not Completed 11 - 7th Year (Old) 12 - Other - 11th Year of Schooling 13 - 2nd year complementary high school course 14 - 10th Year of Schooling 18 - General commerce course 19 - Basic Education 3rd Cycle (9th/10th/11th Year) or Equiv. 20 - Complementary High School Course 22 - Technical-professional course 25 - Complementary High School Course - not concluded 26 - 7th year of schooling 27 - 2nd cycle of the general high school course 29 - 9th Year of Schooling - Not Completed 30 - 8th year of schooling 31 - General Course of Administration and Commerce 33 - Supplementary Accounting and Administration 34 - Unknown 35 - Can't read or write 36 - Can read without having a 4th year of schooling 37 - Basic education 1st cycle (4th/5th year) or equiv. 38 - Basic Education 2nd Cycle (6th/7th/8th Year) or Equiv. 39 - Technological specialization course 40 - Higher education - degree (1st cycle) 41 - Specialized higher studies course 42 - Professional higher technical course 43 - Higher Education - Master (2nd cycle) 44 - Higher Education - Doctorate (3rd cycle|\n",
    "|Mother's occupation|Feature|Discrete|int|0 &ndash; Student 1 &ndash;  Representatives of the Legislative Power and Executive Bodies, Directors, Directors and Executive Managers 2 &ndash;  Specialists in Intellectual and Scientific Activities 3 &ndash;  Intermediate Level Technicians and Professions 4 &ndash;  Administrative staff 5 &ndash;  Personal Services, Security and Safety Workers and Sellers 6 &ndash;  Farmers and Skilled Workers in Agriculture, Fisheries and Forestry 7 &ndash;  Skilled Workers in Industry, Construction and Craftsmen 8 &ndash;  Installation and Machine Operators and Assembly Workers 9 &ndash;  Unskilled Workers 10 &ndash;  Armed Forces Professions 90 &ndash; Other Situation <br>99&ndash;  (blank) 122 &ndash;  Health professionals 123 - teachers 125 - Specialists in information and communication technologies (ICT) 131 - Intermediate level science and engineering technicians and professions 132 - Technicians and professionals, of intermediate level of health 134 - Intermediate level technicians from legal, social, sports, cultural and similar services 141 - Office workers, secretaries in general and data processing operators 143 - Data, accounting, statistical, financial services and registry-related operators 144 - Other administrative support staff 151 - personal service workers 152 - sellers 153 - Personal care workers and the like 171 - Skilled construction workers and the like, except electricians 173 - Skilled workers in printing, precision instrument manufacturing, jewelers, artisans and the like 175 - Workers in food processing, woodworking, clothing and other industries and crafts 191 - cleaning workers 192 - Unskilled workers in agriculture, animal production, fisheries and forestry 193 - Unskilled workers in extractive industry, construction, manufacturing and transport 194 - Meal preparation assistants|\n",
    "|Father's occupation|Feature|Discrete|int|0 &ndash; Student 1 &ndash; Representatives of the Legislative Power and Executive Bodies, Directors, Directors and Executive Managers <br>2 &ndash; Specialists in Intellectual and Scientific Activities 3 &ndash;Intermediate Level Technicians and Professions 4 &ndash; Administrative staff <br>5 &ndash; Personal Services, Security and Safety Workers and Sellers 6 &ndash; Farmers and Skilled Workers in Agriculture, Fisheries and Forestry 7 &ndash; Skilled Workers in Industry, Construction and Craftsmen 8 &ndash; Installation and Machine Operators and Assembly Workers 9 &ndash; Unskilled Workers 10 &ndash; Armed Forces Professions 90 &ndash; Other Situation 99 &ndash; (blank) 101 &ndash; Armed Forces Officers <br>102 &ndash; Armed Forces Sergeants 103 &ndash; Other Armed Forces personnel 112 &ndash; Directors of administrative and commercial services 114 &ndash; Hotel, catering, trade and other services directors 121 - Specialists in the physical sciences, mathematics, engineering and related techniques 122 - Health professionals 123 - teachers 124 - Specialists in finance, accounting, administrative organization, public and commercial relations 131 - Intermediate level science and engineering technicians and professions 132 - Technicians and professionals, of intermediate level of health 134 - Intermediate level technicians from legal, social, sports, cultural and similar services 135 - Information and communication technology technicians 141 - Office workers, secretaries in general and data processing operators 143 - Data, accounting, statistical, financial services and registry-related operators 144 - Other administrative support staff 151 - personal service workers 152 - sellers 153 - Personal care workers and the like 154 - Protection and security services personnel 161 - Market-oriented farmers and skilled agricultural and animal production workers 163 - Farmers, livestock keepers, fishermen, hunters and gatherers, subsistence 171 - Skilled construction workers and the like, except electricians 172 - Skilled workers in metallurgy, metalworking and similar 174 - Skilled workers in electricity and electronics 175 - Workers in food processing, woodworking, clothing and other industries and crafts 181 - Fixed plant and machine operators 182 - assembly workers 183 - Vehicle drivers and mobile equipment operators 192 - Unskilled workers in agriculture, animal production, fisheries and forestry 193 - Unskilled workers in extractive industry, construction, manufacturing and transport <br>194 - Meal preparation assistants 195 - Street vendors (except food) and street service providers|\n",
    "|Admission grade|Feature|Continuous|float|Admission grade (between 0 and 200)|\n",
    "|Displaced|Feature|Discrete|int|1 &mdash; yes 0 &mdash; no|\n",
    "|Educational special needs|Feature|Discrete|int|1 &mdash; yes 0 &mdash; no|\n",
    "|Debtor|Feature|Discrete|int|1 &mdash; yes 0 &mdash; no|\n",
    "|Tuition fees up to date|Feature|Discrete|int|1 &mdash; yes 0 &mdash; no|\n",
    "|Gender|Feature|Discrete|int|1 &mdash; male 0 &mdash; female|\n",
    "|Scholarship holder|Feature|Discrete|int|1 &mdash; yes 0 &mdash; no|\n",
    "|Age at enrollment|Feature|Discrete|int|Age of studend at enrollment|\n",
    "|Scholarship holder|Feature|Discrete|int|1 &mdash; yes 0 &mdash; no|\n",
    "|International|Feature|Discrete|int|1 &mdash; yes 0 &mdash; no|\n",
    "|Curricular units 1st sem (credited)|Feature|Discrete|int|Number of curricular units credited in the 1st semester|\n",
    "|Curricular units 1st sem (enrolled)|Feature|Discrete|int|Number of curricular units enrolled in the 1st semester|\n",
    "|Curricular units 1st sem (evaluations)|Feature|Discrete|int|Number of evaluations to curricular units in the 1st semester|\n",
    "|Curricular units 1st sem (approved)|Feature|Discrete|int|Number of curricular units approved in the 1st semester|\n",
    "|Curricular units 1st sem (grade)|Feature|Discrete|float|Grade average in the 1st semester (between 0 and 20)|\n",
    "|Curricular units 1st sem (without evaluations)|Feature|Discrete|int|Number of curricular units without evalutions in the 1st semester|\n",
    "|Curricular units 2nd sem (credited)|Feature|Discrete|int|Number of curricular units credited in the 2nd semester|\n",
    "|Curricular units 2nd sem (enrolled)|Feature|Discrete|int|Number of curricular units enrolled in the 2nd semester|\n",
    "|Curricular units 2nd sem (evaluations)|Feature|Discrete|int|Number of evaluations to curricular units in the 2nd semester|\n",
    "|Curricular units 2nd sem (approved)|Feature|Discrete|int|Number of curricular units approved in the 2nd semester|\n",
    "|Curricular units 2nd sem (grade)|Feature|Discrete|float|Grade average in the 2nd semester (between 0 and 20)|\n",
    "|Curricular units 2nd sem (without evaluations)|Feature|Discrete|int|Number of curricular units without evalutions in the 1st semester|\n",
    "|Unemployment rate|Feature|Continuous|float|Unemployment rate (%)|\n",
    "|Inflation rate|Feature|Continuous|float|Inflation rate (%)|\n",
    "|GDP|Feature|Continuous|float|GDP|\n",
    "|Target|Target|Categorical|string|Target. The problem is formulated as a three category classification task (dropout, enrolled, and graduate) at the end of the normal duration of the course|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923b51f",
   "metadata": {},
   "source": [
    "## Obtain\n",
    "### Import libraries and Visualization Packages\n",
    "Importing libraries at the beginning allows access to modules and other tools throughout this project that help to make the tasks within this project manageable to implement. The main libraries that will be used within this project include:\n",
    "\n",
    "* <code>pandas</code>: a data analysis and manipulation library which allows for flexible reading, writing, and reshaping of data\n",
    "* <code>numpy</code>: a key library that brings the computationaly power of languages like C to Python\n",
    "* <code>matplotlib</code>: a comprehensive visualization library\n",
    "* <code>seaborn</code>: a data visualization library based on matplotlib\n",
    "* <code>statsmodels</code>: a library used to understand statistical relationships between variables, often used in the field of economics.\n",
    "* <code>selenium</code>:a library used to perform web testing tasks, useful for scraping data \n",
    "* <code>geopy</code>: a library that converts addresses in to geographical locations and vice versa\n",
    "* <code>requests</code>: a library that make HTTP requests and interacts with web services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c2a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and visualization packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "import requests\n",
    "import json\n",
    "import warnings\n",
    "warnings.simplefilter(action ='ignore', category = DeprecationWarning)\n",
    "warnings.simplefilter(action ='ignore', category = FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pandas\")\n",
    "\n",
    "\n",
    "# Allow plots to display and be stored inline within a notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Used for working with the z-score \n",
    "from scipy import stats\n",
    "\n",
    "# Used for working with long url\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "# Set display option to readable format\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "# Filter warnings from pandas\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9408f174",
   "metadata": {},
   "source": [
    "### Check library versions:\n",
    "\n",
    "One of the important parts of modeling is the ability to reproduce results. Science is indeed model dependent and those models improve as more information is gathered over time. Communicating the version of the tools that were used to generate a model is an important part of collaboration. So here we take note of the version of key libraries so that other researchers can have a place to start when attempting to reproduce this work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Pandas version\n",
    "print (\"Pandas version:\")\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c505c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Numpy version\n",
    "print(\"Numpy version:\")\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Seaborn version\n",
    "print(\"Seaborn version:\")\n",
    "sns.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235fc785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Statsmodel version\n",
    "print(\"Statsmodel version:\")\n",
    "sm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef8e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObtainData:\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Constructor for the ObtainData class.\n",
    "        \n",
    "        Parameters:\n",
    "        - data_path(string): Path to the dataset file.\n",
    "        \n",
    "        Initializes the data_path attribute and sets data to None.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.data = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads the dataset from the specified data_path using pandas.\n",
    "        Assumes the dataset is in csv format with semicolons(;) as the separator.\n",
    "        Assigns the loaded data to the data attribute.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(self.data_path, sep = ';')\n",
    "    \n",
    "    def obtain_data(self):\n",
    "        \n",
    "        self.load_data()\n",
    "        return self.data\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ba35f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Main excution\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Initialize the ObtainData object\n",
    "    data_obtained = ObtainData(data_path = 'data/data.csv')\n",
    "    \n",
    "    # Obtain the data\n",
    "    data_obtained.obtain_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a2af1e",
   "metadata": {},
   "source": [
    "## Scrub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b47d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrubData:\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Constructor for the ScrubData class.\n",
    "        \n",
    "        Parameters:\n",
    "        - data (pandas DataFrame): The dataset to be explored\n",
    "        \n",
    "        Initializes the data attribute.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = data\n",
    "\n",
    "    \n",
    "    def check_placeholders(self):\n",
    "        \"\"\"\n",
    "        Checks and displays the presence of placeholders throughout the entire dataframe.\n",
    "        Placeholders checked: '?', '#', 'NaN','null, 'N/A,'-'.\n",
    "        \"\"\"\n",
    "        placeholders = ['?', '#', 'NaN', 'null', 'N/A', '-']\n",
    "        placeholder_mask = self.data.isin(placeholders)\n",
    "        \n",
    "        if placeholder_mask.any().any():\n",
    "            print('\\nPlaceholders Detected:')\n",
    "            display(placeholder_mask)\n",
    "        else:\n",
    "            print('\\nNo Placeholders Detected.')\n",
    "            \n",
    "    def clean(self):\n",
    "        \"\"\"\n",
    "        Perform data cleaning operations.\n",
    "        \"\"\"\n",
    "        cleaned_data = self.data.dropna() # Drop rows with missing data\n",
    "        \n",
    "        return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5668d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Obtained Data Class\n",
    "data = data_obtained.obtain_data()\n",
    "\n",
    "scrubbed_data = ScrubData(data = data)\n",
    "\n",
    "cleaned_data = scrubbed_data.clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543bf95",
   "metadata": {},
   "source": [
    "### Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb962fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExploreData:\n",
    "    def __init__(self,data):\n",
    "        \"\"\"\n",
    "        Constructor for the ExploreData class.\n",
    "        \n",
    "        Parameters:\n",
    "        - data (pandas DataFrame): The dataset to be explored.\n",
    "        \n",
    "        Initializes the data attribute\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "            \n",
    "    def examine_structure(self):\n",
    "        \"\"\"\n",
    "        Prints the shape of the dataset and displays the first few rows.\n",
    "        Also displays the data information including the number of rows and columns in the dataset.\n",
    "        \"\"\"\n",
    "        print('Data Shape:')\n",
    "        display(self.data.shape)\n",
    "        \n",
    "        print('Data Structure:')\n",
    "        display(self.data.head())\n",
    "        \n",
    "        print('\\nData Information:')\n",
    "        display(self.data.info())\n",
    "        \n",
    "    def check_duplicates(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Checks and displays the number of duplicate rows in the dataset.\n",
    "        \"\"\"\n",
    "        num_duplicates = self.data_duplicated().sum()\n",
    "        print (f'\\nNumber of Duplicate Rows: {num_duplicates}')\n",
    "        \n",
    "    def drop_duplicates(self):\n",
    "        \"\"\"\n",
    "        Drops the duplicate rows from the dataset.\n",
    "        \"\"\"\n",
    "        self.data.drop_duplicates(inplace = True)\n",
    "        print(\"Duplicate rows dropped.\")\n",
    "        \n",
    "    def generate_correlation_map(self, title):\n",
    "        \"\"\"\n",
    "        Generates and displays a correlation matrix heatmap for the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -title(str): The title of the correlation map\n",
    "        \"\"\"\n",
    "        \n",
    "        correlation_matrix = self.data.corr()\n",
    "        \n",
    "        # Create a figure and set the title\n",
    "        plt.figure(figsize = (30,25))\n",
    "        plt.suptitle(title)\n",
    "        \n",
    "        # Generate the correlation heatmap\n",
    "        sns.heatmap(correlation_matrix, annot = True, cmap = 'Blues')\n",
    "        \n",
    "        # Show the correlation map\n",
    "        plt.show()\n",
    " \n",
    "    def plot_pairplot(self, correlation_table):\n",
    "        # Get the columns from the correlation table\n",
    "        columns = list(correlation_table.index.levels[0])\n",
    "        \n",
    "        # Filter the data based on the columns\n",
    "        data_filtered = self.data[columns]\n",
    "        \n",
    "        # Create a pairplot using the filtered data\n",
    "        pairplot = sns.pairplot(data_filtered, diag_kind=\"kde\", markers=\".\", height=2)\n",
    "        \n",
    "        # Customize the scatter matrix plot\n",
    "        for ax in pairplot.axes.flat:\n",
    "            ax.xaxis.label.set_rotation(90)\n",
    "            ax.yaxis.label.set_rotation(0)\n",
    "            ax.yaxis.label.set_ha('center')\n",
    "            ax.get_yaxis().set_label_coords(-0.5, 0.5)\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=18)\n",
    "            ax.set_yticklabels(ax.get_yticklabels(), rotation=0, ha='center', fontsize=18)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def generate_correlation_table(self, threshold_min = 0.6, threshold_max = 1.0):\n",
    "        \"\"\"\n",
    "        Generates a correlation table with filtered correlation pairs within specified thresholds.\n",
    "        \n",
    "        Parameters:\n",
    "        - threshold_min (float): The minimum threshold for correlation values (inclusive). Default is 0.6.\n",
    "        - threshold_max (float): The maximum threshold for correlation values (exclusive). Default is 1.0.\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        - filtered_df (pandas DataFrame): The filtered correlation table.\n",
    "        \"\"\"\n",
    "        \n",
    "        correlation_matrix = self.data.corr().abs()\n",
    "        \n",
    "        correlation_df = correlation_matrix.stack().reset_index()\n",
    "        correlation_df.columns = ['feature1', 'feature2', 'correlation']\n",
    "        \n",
    "        filtered_df = correlation_df[\n",
    "            (correlation_df['correlation'] > threshold_min) & (correlation_df['correlation'] < threshold_max)\n",
    "        ]\n",
    "        \n",
    "        filtered_df.drop_duplicates(inplace = True)\n",
    "        filtered_df.set_index(['feature1', 'feature2'], inplace = True)\n",
    "        filtered_df.columns = ['correlation']\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    def plot_gender_distribution(self):\n",
    "        \"\"\"\n",
    "        Plots the gender distribution from the dataset.\n",
    "        \"\"\"\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        gender_counts = self.data['Gender'].value_counts()\n",
    "        gender_labels = ['Female', 'Male']\n",
    "        gender_values = [gender_counts[0], gender_counts[1]]\n",
    "        \n",
    "        plt.figure(figsize = (8,6))\n",
    "        ax = sns.barplot(x = gender_labels, y = gender_values, color = 'blue')\n",
    "                \n",
    "        plt.xlabel('Gender', fontsize = 14, weight = 'bold')\n",
    "        plt.ylabel('Number of Students', fontsize = 14, weight = 'bold')\n",
    "        plt.title('Gender Distribution of College Students',fontsize = 18, weight = 'bold')\n",
    "                \n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def plot_dropout_by_gender(self):\n",
    "        \"\"\"\n",
    "        Plots the gender distribution of students who dropout from the dataset.\n",
    "        \"\"\"\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        dropout_data = self.data[self.data['Target']=='Dropout']\n",
    "        dropout_data['Gender']= dropout_data['Gender'].map({1:'Male', 0:'Female'})\n",
    "\n",
    "        \n",
    "        plt.figure(figsize = (8,6))\n",
    "        ax = sns.countplot(x = 'Gender', data = dropout_data, color = 'blue')\n",
    "                \n",
    "        plt.xlabel('Gender', fontsize = 14, weight = 'bold')\n",
    "        plt.ylabel('Number of Students', fontsize = 14, weight = 'bold')\n",
    "        plt.title('Distribution of Dropouts by Gender',fontsize = 18, weight = 'bold')\n",
    "                \n",
    "        plt.show()        \n",
    "        \n",
    "    def plot_target_distribution(self):\n",
    "        \"\"\"\n",
    "        Plots the gender distribute of the target variable from the dataset.\n",
    "        \"\"\"\n",
    "    \n",
    "        target_data = self.data['Target']\n",
    "        target_labels = ['Dropout', 'Enrolled', 'Graduate']\n",
    "  \n",
    "        \n",
    "        plt.figure(figsize = (8,6))\n",
    "        ax = sns.countplot(x = target_labels, data = target_data, color = 'blue')\n",
    "                \n",
    "        plt.xlabel('Status', fontsize = 14, weight = 'bold')\n",
    "        plt.ylabel('Percentage of Students', fontsize = 14, weight = 'bold')\n",
    "        plt.title('Progress to Graduation Distribution of College Students',fontsize = 18, weight = 'bold')\n",
    "        \n",
    "        plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d2281",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Review correlations on dataset\n",
    "\n",
    "data_exploration = ExploreData(data = cleaned_data)\n",
    "data_exploration.examine_structure()\n",
    "data_exploration.generate_correlation_map(\"Correlation Between Student Academic Success Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9115c85",
   "metadata": {},
   "source": [
    "This seems complicated, so let's simplify it to something that is a bit more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_table = data_exploration.generate_correlation_table(0.6, 1.0)\n",
    "display(correlation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e568d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Plot pairplot using the correlation table\n",
    "data_exploration.plot_pairplot(correlation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bab278",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Plot the gender distribution of the data\n",
    "    data_exploration.plot_gender_distribution()\n",
    "    \n",
    "    # Plot the dropout distribution by gender\n",
    "    data_exploration.plot_dropout_by_gender()\n",
    "    \n",
    "    # Plote the Progress to graduation distribution by gender\n",
    "    data_exploration.plot_target_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb5dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyzeData:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def analyze(self):\n",
    "        # Map the revised target variable values to their corresponding labels\n",
    "        target_labels = {\n",
    "            \"Enrolled\": \"Enrolled\",\n",
    "            \"Graduate\": \"Graduated\",\n",
    "            \"Dropout\": \"Dropout\"\n",
    "        }\n",
    "\n",
    "        # Map the revised marital status variable values to their corresponding labels\n",
    "        marital_status_labels = {\n",
    "            1: \"Single\",\n",
    "            2: \"Married\",\n",
    "            3: \"Widower\",\n",
    "            4: \"Divorced\",\n",
    "            5: \"Facto Union\",\n",
    "            6: \"Legally Separated\"\n",
    "        }\n",
    "\n",
    "        # Filter the data for relevant columns\n",
    "        marital_status_data = self.data[['Marital status', 'Target']]\n",
    "\n",
    "        # Replace the target variable values with their corresponding labels\n",
    "        marital_status_data['Target'] = marital_status_data['Target'].map(target_labels)\n",
    "\n",
    "        # Replace the marital status variable values with their corresponding labels\n",
    "        marital_status_data['Marital status'] = marital_status_data['Marital status'].map(marital_status_labels)\n",
    "\n",
    "        # Plot the results using a bar plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.set_theme(style = \"darkgrid\")\n",
    "        sns.countplot(data=marital_status_data, x='Marital status', hue='Target', palette= 'cool')\n",
    "\n",
    "        # Set plot title and axis labels\n",
    "        plt.title(\"Influence of Marital Status on Graduation Status\")\n",
    "        plt.xlabel(\"Marital Status\")\n",
    "        plt.ylabel(\"Number of Students\")\n",
    "\n",
    "        # Show the plot\n",
    "        plt.legend(title=\"Graduation Status\")\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "# Instantiate the AnalyzeData class with the data\n",
    "marital_status_analysis = AnalyzeData(data=data_exploration.data)\n",
    "\n",
    "# Perform the analysis\n",
    "marital_status_analysis.analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3b204d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ec2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TernaryClassifier:\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Constructor for the TernaryClassifier class.\n",
    "        \n",
    "        Parameters:\n",
    "        - data (pandas DataFrame): The dataset for the classifier.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = data\n",
    "        self.scaler = StandardScaler()\n",
    "        # Define a library of models\n",
    "        self.models = {\n",
    "            'Logistic Regression': LogisticRegression(),\n",
    "            'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "            'Support Vector Machine': SVC(),\n",
    "            'Decision Trees': DecisionTreeClassifier(),\n",
    "            'Random Forest': RandomForestClassifier()\n",
    "        }\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocesses the data by applying one-hot encoding to the target variable and dropping\n",
    "        it from the dataset.\n",
    "        \n",
    "        Returns: \n",
    "        - X (pandas DataFrame): The feature data after preprocessing.\n",
    "        - y (pandas DataFrame): The encoded target variable after  preprocessing\n",
    "        \"\"\"\n",
    "        \n",
    "        # Separate features (X) and target variable (y)\n",
    "        X = self.data.drop('Target', axis = 1)\n",
    "        y = self.data['Target']\n",
    "        \n",
    "        # Perform one-hot encoding on the target variable\n",
    "        y_encoded = pd.get_dummies(y)\n",
    "        \n",
    "        # Scale the input features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Encode the target variable\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(y)\n",
    "        # Drop the original target variable from the dataset\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns = X.columns) # Convert to DataFrame\n",
    "        \n",
    "        return X_scaled_df, y_encoded\n",
    "    \n",
    "    def train_model(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the ternary classifier model on the preprocessed data.\n",
    "        \n",
    "        Parameters:\n",
    "        \n",
    "        - X (pandas DataFrame): The preprocessed feature data.\n",
    "        - y (pandas DataFrame): The preprocessed target variable.\n",
    "        \n",
    "        Returns:\n",
    "        - model: The trained ternary classifier model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define a library of models\n",
    "        models = {\n",
    "            'Logistic Regression': LogisticRegression(),\n",
    "            'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "            'Support Vector Machine': SVC(),\n",
    "            'Decision Trees': DecisionTreeClassifier(),\n",
    "            'Random Forest': RandomForestClassifier()\n",
    "        }\n",
    "        # Train each model in the library\n",
    "        for model_name, model in models.items():\n",
    "            model.fit(X,y)\n",
    "        \n",
    "        # Store the trained models\n",
    "        self.models = models\n",
    "        \n",
    "        # Return the trained models dictionary\n",
    "        return models\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predicts the target variable values using all the trained models.\n",
    "\n",
    "        Parameters:\n",
    "        - X_test (pandas DataFrame): Testing feature data.\n",
    "\n",
    "        Returns:\n",
    "        - predictions (dict): Dictionary containing the predictions of each model.\n",
    "        \"\"\"\n",
    "        predictions = {}\n",
    "\n",
    "        for model_name, model in self.models.items():\n",
    "            model_predictions = model.predict(X_test)\n",
    "            predictions[model_name] = model_predictions\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1148f17c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, models, X_test, y_test):\n",
    "        \"\"\" \n",
    "        Constructor for the ModelEvaluation class.\n",
    "        \n",
    "        Parameters: \n",
    "        - models (list): List of model names.\n",
    "        - X_test (pandas DataFrame): Testing feature data.\n",
    "        - y_test (pandas Series): Testing target variable data.\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.evaluation_df = None\n",
    "        \n",
    "    def analyze_feature_importances(self, trained_models, feature_names):\n",
    "        \"\"\"\n",
    "        Analyze the feature importances of the trained models.\n",
    "        \n",
    "        Parameters: \n",
    "        - trained_models (dict): Dictionary containing trained machine learning models.\n",
    "        - feature_names(list): List of feature names used in the models.\n",
    "        \n",
    "        Returns:\n",
    "        - None (The results will be stored in the evaluation_df (pandas DataFrame))\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create a DataFrame\n",
    "        feature_importances_df = pd.DataFrame(index = feature_names)\n",
    "        \n",
    "        # Analyze feature importance in each model\n",
    "        for mode_name, model in trained_models.items():\n",
    "            if isinstance(model, RandomForestClassifier):\n",
    "                # Get the feature importances from the Random Forest Classifier\n",
    "                feature_importances = model.feature_importances_\n",
    "                feature_importances_df[model_name] = feature_importances\n",
    "        \n",
    "        # Add the feature importances DataFrame to the evaluation_df\n",
    "        self.evaluation_df = pd.concat([self.evaluation_df, feature_importances_df], axis=1)\n",
    "        \n",
    "    def evaluate_models(self, classifier):\n",
    "        \"\"\"\n",
    "        Evaluate the performance of trained models using appropriate evaluation metrics.\n",
    "\n",
    "        Parameters:\n",
    "        - classifier (TernaryClassifier): The TernaryClassifier instance.\n",
    "\n",
    "        Returns:\n",
    "        - evaluation_df (pandas DataFrame): DataFrame with evaluation metrics for each model.\n",
    "        \"\"\"\n",
    "        accuracy = []\n",
    "        precision = []\n",
    "        recall = []\n",
    "        f1_scores = []\n",
    "\n",
    "        # Evaluate each model in the TernaryClassifier\n",
    "        for model_name, model in classifier.models.items():\n",
    "            predictions = classifier.predict(self.X_test)\n",
    "\n",
    "            # Calculate evaluation metrics\n",
    "            acc = accuracy_score(self.y_test, predictions[model_name])\n",
    "            prec = precision_score(self.y_test, predictions[model_name], average='weighted', zero_division=1)\n",
    "            rec = recall_score(self.y_test, predictions[model_name], average='weighted')\n",
    "            f1 = f1_score(self.y_test, predictions[model_name], average='weighted')\n",
    "\n",
    "            # Append metrics to respective lists\n",
    "            accuracy.append(acc)\n",
    "            precision.append(prec)\n",
    "            recall.append(rec)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        # Create the evaluation dataframe\n",
    "        self.evaluation_df = pd.DataFrame({\n",
    "            'Model': list(classifier.models.keys()),\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1_scores\n",
    "        })\n",
    "        \n",
    "        # Analyze feature importances\n",
    "        self.analyze_feature_importances(trained_models, feature_names)\n",
    "        return self.evaluation_df\n",
    "    \n",
    "    def tune_parameters(self, classifier, param_grid, scorer):\n",
    "        \"\"\"\n",
    "        Perform grid search to tune hyperparameters of the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        - classifier: An instance of a classifier.\n",
    "        - param_grid (dict): A dictionary specifying the hyperparameter grid.\n",
    "        - scorer: The scoring function to be used.\n",
    "        \n",
    "        Returns:\n",
    "        - best_params (dict): A dictionary containig the best Hyperparameters\n",
    "        - best_score: The best score achieved by the model after hyperparameters were tuned.\n",
    "        \"\"\"\n",
    "        grid_search = GridSearchCV(estimator = classifier, param_grid = param_grid, scoring = scorer, cv = 5)\n",
    "        grid_search.fit(self.X_test, self.y_test)\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "        return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45611cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the TernaryClassifier class\n",
    "ternary_classifier = TernaryClassifier(data=cleaned_data)\n",
    "\n",
    "# Preprocess the data\n",
    "X_scaled, y_encoded = ternary_classifier.preprocess_data()\n",
    "\n",
    "# Perform train-test split with 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the models using the train_model method\n",
    "trained_models = ternary_classifier.train_model(X_train, y_train)\n",
    "\n",
    "# Instantiate the ModelEvaluation class\n",
    "model_evaluation = ModelEvaluation(models=['Logistic Regression', 'Decision Tree', 'Random Forest',\n",
    "                                           'K-Nearest Neighbors', 'Support Vector Machine'],\n",
    "                                   X_test=X_test, y_test=y_test)\n",
    "\n",
    "# Evaluate models and store the results in a dataframe\n",
    "evaluation_results = model_evaluation.evaluate_models(classifier=ternary_classifier)\n",
    "\n",
    "# Display the evaluation dataframe\n",
    "display(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8483dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50,100, 150, 200, 250,  300],\n",
    "    'max_depth': [None, 5, 10, 15, 20, 25 ],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create the RandomForestClassifier instance\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# Create the recall scorer\n",
    "scorer = make_scorer(recall_score, average='weighted')\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params, best_score = model_evaluation.tune_parameters(RandomForestClassifier(), param_grid, scorer)\n",
    "\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score (Recall):\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3951db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "318.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
